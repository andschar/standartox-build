\section{Methods}
An automated processing pipeline downloads the quarterly released ECOTOX database, performs several preparation steps on it and exports a final Standartox data set. This data set is accessible via a web application and an application programming interface (API). An API provides the means for machine communication between a host and a client and thus allows scriptable data queries. To facilitate the API access, the R \citep{rcoreteam_language_2017} package \textit{standartox} is built. All data presented in this paper are derived from the Standartox build based on the ECOTOX release from the 12.12.2019.

\subsection{Processing}
Standartox downloads the quarterly released EPA ECOTOX database and builds it into a local PostgreSQL database \citep{szocs_build_2019}. Subsequently, Structured Query Language (SQL) functions for further processing the data are implemented. In addition lookup tables (Supplement \ref{sup:conv-concentration}, \ref{sup:conv-duration}) that enable duration and concentration unit conversions of the data are created. A meta table providing information such as the release version of the ECOTOX database is added. Then, provided Chemical Abstracts Service Numbers (CAS) and taxonomic names are used to query additional information from publicly available databases on chemicals and organisms, respectively. This includes the Compendium of Pesticide Common Names \citep{wood_compendium_2019}, the Chemical Entities of Biological Interest (ChEBI) database \citep{hastings_chebi_2016}, the Chemical Identifier Resolver (CIR) service \citep{nationalinstitutesofhealthnih_chemical_2019}, the Pubchem database \citep{kim_pubchem_2016}, Eurostat \citep{europeancommission_eurostat_2019} and Wikidata \citep{vrandecic_wikidata_2014} for chemicals and the World Register of Marine Species (WoRMS) \citep{wormseditorialboard_world_2018} and the Global Biodiversity Information Facility (GBIF) \citep{gbif_gbif_2019} (Table \ref{tab:data-base-additional}) for habitat and spatial distribution of organisms. Given that CAS and taxonomic names are not always unambiguous, e.g. the genus \textit{Eisenia} can refer to an algae and a worm at once, we first match them against the database specific identifiers and subsequently use them to retrieve the actual data. In a next step the data is added to Standartox to enable filtering for specific chemical roles (e.g. drug, pesticide, personal care product) and classes (e.g. pyrethroid, metal, carbamate) as well as spatial distribution (i.e. continents) and habitat preferences (e.g. freshwater) of individual taxa. Taxa that where not identified to at least genus level are excluded. Finally, the Standartox data set is compiled, which includes the harmonisation of data, e.g. through conversion of result concentration and test duration units. 1237 distinct concentration units are converted to X harmonised ones, as are the 126 distinct duration units to X. Standartox retains only duration units, that can unambiguously be converted to hours, excluding duration units such as harvest or lifetime. To guarantee appropriate unit conversion and harmonisation, we compared one automated conversion to a manually done conversion for each of the distinct concentration and duration units. Furthermore, the units are cleaned, for example through removing additional information in the field such as \textit{food}, \textit{soil}, \textit{ai} that are also coded in other variables and hinder the processing of units. Concentrations that are given as rates such as per day (e.g. mg/kg/day) are multiplied by the days and the converted. Experimental endpoints are restricted to three groups, namely NOEX, LOEX and XX50. Other endpoints, such as Bioconcentration factors, non-half maximal effective concentrations (e.g. IC10, EC25, LD99) or maximum acceptable toxicant (MATC) concentrations are removed. Overall this reduces the ECOTOX data set from 971,430 to 611,248 harmonised test results that are included in Standartox. Along with that, a catalog, listing all distinct entries and value ranges, for categorical and continuous variables, respectively is created. Finally, we run quality control scripts that check the accuracy of the data. Single processing scripts are listed in Figure \ref{fig:pipeline-tree}. The final Standartox table together with the catalog is exported and accessible via the web application and the API, through the R package.



\begin{figure}
    \includestandalone[scale=0.5]{article/tikz/pipeline-organigram}
    %\input[scale = 0.5]{article/tikz/pipeline-organigram.tex}
    \caption{Organigram of Standartox.}
    \label{fig:stx-organigram}
\end{figure}

\subsection{Application methods}
The web application and the API load the compressed serialized Standartox data into memory and allow a client to interact with it. The client thereby calls the functions stx\_filter() and stx\_aggregate() that filter and aggregate the data according to specific parameters (Table \ref{tab:app-parameters}). The interactive web application is built in R using the shiny framework, which runs with the help of a shiny server \citep{chang_shiny_2018} on \url{standartox.uni-landau.de}. The API is built by using the R package \textit{plumber} \citep{trestletechnologyllc_plumber_2018}, which allows for the creation of Representational State Transfer (REST) APIs from R. REST is a software standard that defines web service communication rules. The API is reachable via the URL \url{http://139.14.20.252} and port 8000. Four API-endpoints (\textit{/catalog}, \textit{/filter}, \textit{/aggregate} and \textit{/meta}) can be queried by users. The \textit{/catalog} API-endpoint returns a JavaScript Object Notation (JSON) file containing a catalog of possible filter parameters to choose from. The \textit{/filter} returns the filtered Standartox table as a compressed serialized binary file created by the R \textit{fst} package \citep{klik_fst_2019}, to reduce size and allow for fast user queries. The \textit{/aggregate} API-endpoint returns an R function allowing the client to aggregate the filtered data. Lastly, the \textit{/meta} API-endpoint returns a JSON file with meta information, such as the timestamp of the request and the used Standartox version. The API is designed to be used with the R-package \textit{standartox} and therefore uses serialization methods specific to R (rds() from the R package \pkg{base} and fst() from the package fst). In order to facilitate the API usage the R-package \textit{standartox} is created.

