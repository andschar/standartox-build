
\section*{Methods}

%%The Methods should include detailed text describing any steps or procedures 
%%used in producing the data, including full descriptions of the experimental 
%%design, data acquisition assays, and any computational processing (e.g. 
%%normalization, image feature extraction). Related methods should be grouped 
%%under corresponding subheadings where possible, and methods should be described 
%%in enough detail to allow other researchers to interpret and repeat, if required, 
%%the full study. Specific data outputs should be explicitly referenced via data 
%%citation (see Data Records and Data Citations, below). Authors should cite 
%%previous descriptions of the methods under use, but ideally the method 
%%descriptions should be complete enough for others to understand and reproduce 
%%the methods and processing steps without referring to associated publications. 
%%There is no limit to the length of the Methods section.

Standartox consists of three software parts which are explained in detail here. Firstly a processing pipeline (Fig. \ref{fig:pipeline}) downloads quarterly released new versions of the EPA ECOTOX data base and performs several preparation steps in R \citep{r_core_team_r_2017}, to then store the outcome in a PostgreSQL <CITE> data base. This constitutes the basis upon which the web application and and application programming interface (API) together with a R package are accessing the data \citep{r_core_team_r_2017}. 

\subsection{}



The application itself is set of R functions that filter and aggregate the data according to a user's inputs \ref{fig:app}. Finally in the web application, a `filtered` and an `aggregated` data set as well as an interactive plot, for data exploration are returned. These two data sets can be downloaded. In the following these two parts are explained in detail, naming the respective R scripts in brackets.

\subsection*{Data acquisition \& preparation}



The EPA ECOTOX data is downloaded and subsequently built into a PostgreSQL data base. Afterwards the data is imported into R for pre-processing, which includes validity checks, removal of special characters, unit harmonizations and selection of relevant variables. Chemical abstract service (CAS) numbers, unique chemical identifiers together with taxon names are then used to query other data bases for additional information on chemicals and organisms respectively. Chemical information is retrieved inter alia from the Compendium of Pesticide Common Names \citep{CITE_AW}, the Chemspider data base \citep{CITE-CHEMSPIDER}, from Eurostat <CITATION>, the Pubchem data base \citep{CITE_PUBCHEM}, and from the Physprop data base \citep{CITE_PHYSPROP}. These queries mostly rely on the webchem R-package \citep{szocs_webchem_2015-1}. For biota, habitat and occurrence information is queried from the World Register of Marine Species (WORMS) \citep{WORMS} and from the Global Biodiversity Information Facility (GBIF) \citep{CITE_RGBIF} using the rgbif R-package \citep{chamberlain_rgbif_2018}. The acquired information is then merged and information is combined. From there on the data set is analyzed and checked and finally compiled in one table. This table is then used as the input for the filter and aggregation functions \ref{fig:pipeline}. For a detailed overview of what information is collected see table \ref{tab:scripts-pipeline}.

\begin{figure}
    \includestandalone[width=\textwidth, scale=0.1]{tikz/pipeline_organigram}
    \caption{Processing Pipeline}
    \label{fig:pipeline}
\end{figure}


\subsection*{The application}
The application is accessible through \app{} and was built in R, using the shiny web application framework \citep{chang_shiny_2018}. Users can define several input parameters \ref{tab:inputs} in the application's graphical user interface (GUI), handled in the ui.R script in order to adjust the aggregation according specific requirements. In doing so users execute R functions (fun\_filter.R, fun\_aggregation.R, fun\_filagg\_plot\_ly.R) loaded in the server.R script, which produce a 'filtered' and an 'aggregated' data set as well as an interactive overview plot. Only the 'aggregated' data set is presented in the GUI. However, both data sets can be downloaded via a download button.

\subsubsection*{Aggregation}
The \etoxbase{} application aggregates the test results according to chosen filters in a two step process. Firstly the filtered test results are aggregated by the CAS number, the chosen taxon and the selected test duration. Secondly, the returned data is then aggregated by the CAS number. The former can't be influenced by the user and calculates either the minimum or the median depending on the amount of results to aggregate (n <= 2: minimum, if n > 2: median). Thereof the second step calculates the minimum, the maximum, the median, the geometric mean, or the arithmetic mean as an aggregate. 

\subsubsection*{Future of the project}
Since new versions of the EPA ECOTOX data base are published on a quarterly basis the data acquisition process is automated and new versions of the Standartox are scheduled to be released regularly.


The code for the web-application and the application programming interface are placed here: https://github.com/andschar/standartox-app

\begin{figure}
    \includestandalone[scale=0.75]{tikz/application}
    \caption{Application}
    \label{fig:app}
\end{figure}

\subsection*{The R package}

The code for the R-package standartox is placed here: https://github.com/andschar/standartox




\subsection*{Code availability}
%%For all studies using custom code in the generation or processing of datasets, 
%%a statement must be included here, indicating whether and how the code can be 
%%accessed, including any restrictions to access. This section should also include 
%%information on the versions of any software used, if relevant, and any specific 
%%variables or parameters used to generate, test, or process the current dataset. 

The processing pipeline code is stored here https://github.com/andschar/standartox-build, the code for the web application is stored under https://github.com/andschar/standartox-app and the R-pckage code is stored in https://github.com/andschar/standartox. The code can freely be accessed under the MIT License <CITATION>. Most of the code is written in R 3.6.1 and some parts in PostgreSQL 9.6.1. An exhaustive list of additional R-pakage used can be found in the supplementary material <REFERENCE-TO-SUPPL>.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|l|l|l|}
        \hline
        Category        & Package       & Version   & Reference                     \\ \hline
        processing      & RCurl         & 1.95-4.11 & \citep{lang_rcurl_2018}       \\ \hline
        processing      & data.table    & 1.12.0    & \citep{dowle_data.table_2018} \\ \hline
        application     & shiny         & 1.1.0     & \citep{chang_shiny_2018}      \\ \hline
    \end{tabular}
    \caption{List of R packages used in compiling the data.\newline{}
    TODO: Automate version input and name generation - whole table\newline{}
    TODO2: debug chang\_shiny\_2018 reference}
    \label{tab:rpackages}
\end{table}


\section{TODO}

    \item it is planed to extend this tool to other data bases if they provide new test data  (e.g. UBA ETOC)